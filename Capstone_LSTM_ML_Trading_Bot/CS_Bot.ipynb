{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forex Factory Scraper\n",
    "\n",
    "import cloudscraper\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime as dt\n",
    "import re\n",
    "import time\n",
    "import calendar\n",
    "import pickle5 as pickle\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from dateutil.parser import *\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import pytz\n",
    "from pytz import timezone\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,GRU,LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = '9ecbc733416d86f9410a748dd4850d0b-25b7ef74dead0b13a9fef52b628f50c7'\n",
    "ACCOUNT_ID = '101-003-19647154-001'\n",
    "\n",
    "OANDA_URL = 'https://api-fxpractice.oanda.com/v3'\n",
    "\n",
    "SECURE_HEADER = {\n",
    "    'Authorization': f'Bearer {API_KEY}',\n",
    "    'Content-Type': 'application/json'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(row):\n",
    "    d= parse(row)\n",
    "    return d\n",
    "    \n",
    "def parse_utc_to_sg(row):\n",
    "    d = parse(row)\n",
    "    sg_time = astimezone(d)\n",
    "    \n",
    "    return sg_time\n",
    "\n",
    "def astimezone(row):\n",
    "    return row.astimezone(pytz.timezone('Singapore'))\n",
    "\n",
    "def astimezone_utc(dt_obj):\n",
    "    return dt_obj.astimezone(pytz.timezone('utc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns the pkl file name of certain pair to standardize\n",
    "def get_his_data_filename(pair, granularity,time_from = None, time_to = None):\n",
    "    \n",
    "    #if want to include time in filename, then input time_from\n",
    "    \n",
    "    if time_from:\n",
    "        \n",
    "        return f\"./datasets/{pair}_{granularity}_{time_from}_{time_to}.pkl\"\n",
    "        \n",
    "    else:\n",
    "        return f\"./datasets/{pair}_{granularity}.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sends slack message in cases of errors or when trade done\n",
    "\n",
    "def slack_msg(text):\n",
    "    url = \"https://hooks.slack.com/services/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\"\n",
    "    message = (text)\n",
    "    title = (f\"New Incoming Message :zap:\")\n",
    "    slack_data = {\n",
    "        \"username\": \"NotificationBot\",\n",
    "        \"icon_emoji\": \":satellite:\",\n",
    "        \"channel\" : \"oanda_notify\",\n",
    "        \"attachments\": [\n",
    "            {\n",
    "                \"color\": \"#9733EE\",\n",
    "                \"fields\": [\n",
    "                    {\n",
    "                        \"title\": title,\n",
    "                        \"value\": message,\n",
    "                        \"short\": \"false\",\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    byte_length = str(sys.getsizeof(slack_data))\n",
    "    headers = {'Content-Type': \"application/json\", 'Content-Length': byte_length}\n",
    "    response = requests.post(url, data=json.dumps(slack_data), headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeDateAdjust(event_time_hour, event_time_minutes, am_or_pm, hours_to_adjust, year, month, day):\n",
    "    \n",
    "    # function to change to 24h format and adjust to local timezone\n",
    "    \n",
    "    # Hours_to_adjust input is used to adjust for timzone differences as the forex factory calendar is in EST\n",
    "    \n",
    "    d = dt.date(year, month, day)\n",
    "    \n",
    "    if(am_or_pm == \"am\"):\n",
    "        adjusted_hour = int(event_time_hour) + hours_to_adjust\n",
    "    \n",
    "    else:\n",
    "    # If pm then add 12 hours to adjust to 24 hours format\n",
    "        adjusted_hour = int(event_time_hour) + 12 + hours_to_adjust\n",
    "    \n",
    "    if (adjusted_hour < 24):\n",
    "    # If adjusted_hour < 24 hours no need to update the date\n",
    "    # if it is over 24 then this means that it is the next day and the date needs to be updated.\n",
    "    \n",
    "        adjusted_time = str(adjusted_hour) + event_time_minutes # Returns string representation of the 24h time in HH:MM\n",
    "        d_of_week = calendar.day_abbr[d.weekday()] # use the calendar API to return Mon-Sun in abbreviated format as a string\n",
    "        d= d.strftime(\"%Y.%m.%d\") # Returns the date as a string in the format YYYY:MM:DD\n",
    "        \n",
    "        # returns date, time and day of week\n",
    "        return (d, adjusted_time, d_of_week)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        adjusted_hour = adjusted_hour - 24 \n",
    "        # If it is PM, Minus 24h as it is now the next day and differenced time will be am time of the next day\n",
    "        \n",
    "        adjusted_time = str(adjusted_hour) + event_time_minutes \n",
    "        # Returns string representation of the 24h time in HHMM\n",
    "        \n",
    "        d = d + dt.timedelta(days=1) \n",
    "        # Adds one day on the original date of the event as it is now new day of local time\n",
    "        \n",
    "        d_of_week = calendar.day_abbr[d.weekday()] # use the calendar API to return Mon-Sun in abbreviated format as a string\n",
    "        d= d.strftime(\"%Y.%m.%d\") # Returns the date as a string in the format YYYY:MM:DD\n",
    "        \n",
    "        # returns date, time and day of week\n",
    "        return (d, adjusted_time, d_of_week)\n",
    "        \n",
    "def strToIntMonth(month):\n",
    "\n",
    "    # Function to convert Str Month into an Int\n",
    "\n",
    "    if (month == 'Jan'):\n",
    "        return 1\n",
    "    elif (month == \"Feb\"):\n",
    "        return 2\n",
    "    elif (month == \"Mar\"):\n",
    "        return 3\n",
    "    elif (month == \"Apr\"):\n",
    "        return 4\n",
    "    elif (month == \"May\"):\n",
    "        return 5\n",
    "    elif (month == \"Jun\"):\n",
    "        return 6\n",
    "    elif (month == \"Jul\"):\n",
    "        return 7\n",
    "    elif (month == \"Aug\"):\n",
    "        return 8\n",
    "    elif (month == \"Sep\"):\n",
    "        return 9\n",
    "    elif (month == \"Oct\"):\n",
    "        return 10\n",
    "    elif (month == \"Nov\"):\n",
    "        return 11\n",
    "    elif (month == \"Dec\"):\n",
    "        return 12\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEventsCalendar(start_date, end_date, file_path):\n",
    "    \n",
    "        # Need cloudscraper to bypass cloudflare\n",
    "        scraper = cloudscraper.create_scraper(allow_brotli=False, disableCloudflareV1=True)\n",
    "\n",
    "        # Gets One Day at a time\n",
    "\n",
    "        url = 'https://www.forexfactory.com/' + start_date\n",
    "\n",
    "        # query the website and return the html to the variable ‘page’\n",
    "        page = scraper.get(url).text\n",
    "\n",
    "        ### print(page)\n",
    "        # parse the html using beautiful soup and store in variable `soup`\n",
    "\n",
    "        soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "        # Find the table containing all the data\n",
    "        table = soup.find('table', class_ = 'calendar__table')\n",
    "\n",
    "        # Get Date of Event\n",
    "        #print(table.find_next('tr', class_ = 'calendar__row--new-day'))\n",
    "        date_of_events = table.find_next('tr', class_ = 'calendar__row--new-day').find_next('span', class_ = 'date')\n",
    "\n",
    "        # Regualr Expression to find the 'day of week', 'month' and the 'day'\n",
    "        matchObj = re.search('([a-zA-Z]{3})([a-zA-Z]{3}) ([0-9]{1,2})', date_of_events.text)\n",
    "\n",
    "        # Assigning the 'day of week', 'month' and 'day'\n",
    "\n",
    "        day_of_week = matchObj.group(1)\n",
    "        month = matchObj.group(2)\n",
    "        month = strToIntMonth(month) # Convert from Str to Int\n",
    "        month = int(format(month, '02')) # Places 0's in front of the month if it is single digit day, for months Jan - Sep\n",
    "\n",
    "        day = matchObj.group(3)\n",
    "        day = int(format(int(day), \"02\")) # Places 0's in front of the day if it is single digit day, for days 1-9 of the month\n",
    "\n",
    "        year = int(start_date[-4:])\n",
    "\n",
    "        date_tentative = np.NaN\n",
    "\n",
    "        # Event Times \n",
    "        event_times = table.find_all('td', class_ = 'calendar__time')\n",
    "\n",
    "        if(day_of_week != 'Sat' and day_of_week != 'Sun'):\n",
    "            for news in event_times:\n",
    "\n",
    "                # currency which will be effected by the event\n",
    "                curr = news.find_next_sibling('td', class_ = 'currency').text.strip()\n",
    "\n",
    "                # impact of event (high,medium,low)\n",
    "                impact = news.find_next_sibling('td', class_ = 'impact').find_next('span')['class']\n",
    "                impact = impact[0]\n",
    "\n",
    "                #event name\n",
    "\n",
    "                event = news.find_next_sibling('td', class_ = 'event').find_next('span').text.strip()\n",
    "\n",
    "                #previous report figures if it is a continuous on-going report e.g. nfp\n",
    "\n",
    "                previous = news.find_next_sibling('td', class_ = 'previous').text\n",
    "\n",
    "                #forecasted figures if it is a continuous on-going report e.g. nfp\n",
    "\n",
    "                forecast = news.find_next_sibling('td', class_ = 'forecast').text\n",
    "\n",
    "                # actual figures if event has happened\n",
    "\n",
    "                actual = news.find_next_sibling('td', class_ = 'actual').text\n",
    "\n",
    "                # get event time\n",
    "                event_time = news.text.strip()\n",
    "\n",
    "                try:\n",
    "                    matchObj = re.search('([0-9]+)(:[0-9]{2})([a|p]m)', event_time) # Regex to match time in the format HH:MMam/pm\n",
    "\n",
    "                    if(matchObj != None):\n",
    "\n",
    "                    # if we get a time, then we store the hour, minutes and am/pm\n",
    "\n",
    "                        event_time_hour = matchObj.group(1) # Matches the first group in the regex which is the hour in HH format\n",
    "                        event_time_minutes = matchObj.group(2) # Matches the second group in the regex which is the minutes in :MM format \n",
    "                        am_or_pm = matchObj.group(3) # Matches the third group in the regex which is either 'am' or 'pm'\n",
    "\n",
    "                    # for events that are all day events, we cannot trade on them and put the time as nan\n",
    "\n",
    "                    elif(re.search('([a-zA-Z]+)', event_time)):\n",
    "                        with open(file_path, 'a') as file:\n",
    "                            file.write('{}, {}, {}, {}, {}, {}, {}, {}, {}\\n'.format(day_of_week, date_tentative, event_time, curr, impact, event, previous, forecast, actual))\n",
    "                        continue\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        # else no time and use previous events time and write to file as ff will not give multiple \n",
    "                        # \\n event times if they are of the same timing\n",
    "\n",
    "                        with open(file_path, 'a') as file:\n",
    "                            file.write('{}, {}, {}, {}, {}, {}, {}, {}, {}\\n'.format(day_of_week, parser(event_date_time), event_time_holder, curr, impact, event, previous, forecast, actual))\n",
    "                        continue\n",
    "\n",
    "                    # adjust the datetime to 24h and local time\n",
    "                    # Returns a tuple with 3 elements consisting of 'event date YYYY:MM:DD', 'event time HH:MM', \n",
    "                    # \\n 'day of week Mon-Fri'\n",
    "\n",
    "                    adjusted_date_time = timeDateAdjust(event_time_hour, event_time_minutes, am_or_pm, 12, year, month, day)\n",
    "\n",
    "                    event_date = adjusted_date_time[0]\n",
    "                    event_time = adjusted_date_time[1]\n",
    "                    day_of_week = adjusted_date_time[2]\n",
    "\n",
    "                    if event_time != '' and event_time != 'All Day': # If the event time is not empy and not 'All day' then we have found a time \n",
    "                        event_time_holder = str(adjusted_date_time[1]) # Set the event_time_holder to this event_time so any other same time events can use this to record\n",
    "                                                                    #\\n the same time as forex factory only provides a time for the first event\n",
    "                        event_date_time = '{} {}'.format(event_date, event_time_holder)\n",
    "                    else:\n",
    "                        event_time_holder = event_time_holder # event_time_holder remains the same and should have the value of the first event which was assigned a time\n",
    "                        event_date_time = '{} {}'.format(event_date, event_time_holder) \n",
    "\n",
    "                except Exception as e:\n",
    "                    print(\"There was an error: \" + e)\n",
    "\n",
    "                ### print(file_path)\n",
    "                print('{}, {}, {}, {}, {}, {}, {}, {}, {}\\n'.format(day_of_week, event_date_time, event_time_holder, curr, impact, event, previous, forecast, actual))\n",
    "                with open(file_path, 'a') as file:\n",
    "                    file.write('{}, {}, {}, {}, {}, {}, {}, {}, {}\\n'.format(day_of_week, event_date_time, event_time_holder, curr, impact, event, previous, forecast, actual))\n",
    "\n",
    "        if start_date == end_date:\n",
    "            print('Successfully retrieved all data')\n",
    "            return True\n",
    "\n",
    "        else:\n",
    "            scrape_next_day = soup.find('div', class_='head').find_next('a', class_='calendar__pagination--next')['href']\n",
    "            getEventsCalendar(scrape_next_day, end_date, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_events():\n",
    "\n",
    "    #take note that if u query too many times, cloudfare might detect and block u. u may have to use vpn then.\n",
    "\n",
    "    event_time_holder = '' # Holds event time of previous news event if it does not have one\n",
    "    \n",
    "    abs_path = os.path.abspath('a')\n",
    "    cwd = os.path.dirname(abs_path) \n",
    "    parent_dir = os.path.dirname(cwd)  \n",
    "    file_path = parent_dir + \"/capstone/datasets/ffc_news_events_2.csv\"\n",
    "    ### file_path = '/Users/User_1/Desktop' + \"/ffc_news_events.csv\"\n",
    "\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True) #create file if file does not exist\n",
    "\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(\"\") # Needs to write an empty line so that file is opened and getEventsCalendar can append to the file\n",
    "\n",
    "    est_tz = timezone('EST') #query in est\n",
    "\n",
    "    est_date_now = dt.datetime.now(est_tz)\n",
    "    \n",
    "    # get next week's events\n",
    "    days_into_future = 8\n",
    "\n",
    "    est_date_future = dt.datetime.now(est_tz)+dt.timedelta(days_into_future)\n",
    "    \n",
    "    date_start = str(calendar.month_abbr[est_date_now.month]) + str(est_date_now.day)+ '.' + str(est_date_now.year)\n",
    "    date_end = str(calendar.month_abbr[est_date_future.month]) + str(est_date_future.day)+ '.' + str(est_date_future.year)\n",
    "\n",
    "    getEventsCalendar(f\"calendar?day={date_start}\".lower(),f\"calendar?day={date_end}\".lower(), file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_events():    \n",
    "    # extracting events from saved file\n",
    "\n",
    "    event_df = pd.read_csv('./datasets/ffc_news_events_2.csv')\n",
    "\n",
    "    event_df.columns = ['day_of_week', 'event_date_time', 'event_time_holder', 'curr', 'impact', 'event', 'previous', 'forecast', 'actual']\n",
    "    \n",
    "    event_df = event_df.select_dtypes(['object']).apply(lambda x: x.str.strip())\n",
    "\n",
    "    # exclude all-day events with no specific time\n",
    "\n",
    "    event_df = event_df[event_df.event_date_time != 'nan']\n",
    "\n",
    "    event_df.reset_index(inplace=True,drop=True)\n",
    "\n",
    "    event_df['event_date_time']= event_df['event_date_time'].apply(parse_utc_to_sg)\n",
    "    \n",
    "    #event_df['event_date_time']= event_df['event_date_time'].apply(astimezone)\n",
    "    \n",
    "    # only filter out the usd events and medium/high impact events\n",
    "\n",
    "    event_df = event_df[( (event_df['curr']=='USD')) & (event_df['impact']!= 'low')]\n",
    "    \n",
    "    event_df.reset_index(inplace=True,drop=True)\n",
    "    \n",
    "    event_df = event_df.loc[event_df['event_date_time'].drop_duplicates(keep='first').index]\n",
    "    \n",
    "    # creating column to mark time for bot to get candles\n",
    "    \n",
    "    event_df['get_candle_time'] = event_df['event_date_time']+dt.timedelta(minutes=9)\n",
    "    \n",
    "    display(event_df.iloc[-1])\n",
    "\n",
    "    event_df.to_pickle('./datasets/ff_event_df_bot.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_up_coming_events():\n",
    "    \n",
    "    ''' stores new events that our bot needs to look out for in a pickle '''\n",
    "    \n",
    "    up_coming_events_df = pd.read_pickle('./datasets/up_coming_events.pkl')\n",
    "    \n",
    "    event_df = pd.read_pickle('./datasets/ff_event_df_bot.pkl')\n",
    "    \n",
    "    event_subset_df = event_df[-100:]\n",
    "    \n",
    "    for i in event_subset_df.index:\n",
    "        time = event_subset_df.loc[i]['get_candle_time']\n",
    "        \n",
    "        # if event not in up_coming_events, store the new event\n",
    "    \n",
    "        if (up_coming_events_df['get_candle_time']==time).any()==False:\n",
    "            up_coming_events_df = up_coming_events_df.append(event_subset_df.loc[i])\n",
    "            \n",
    "    display(up_coming_events_df.iloc[-1])\n",
    "            \n",
    "    up_coming_events_df.to_pickle('./datasets/up_coming_events.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class to draw out existing trades info\n",
    "\n",
    "class OandaTrade():\n",
    "    def __init__(self,oanda_ob):\n",
    "        \n",
    "        #This function is to close trades\n",
    "        \n",
    "        self.unrealizedPL = float(oanda_ob['unrealizedPL'])\n",
    "        self.currentUnits = int(oanda_ob['currentUnits'])\n",
    "        self.trade_id= int(oanda_ob['id'])\n",
    "        self.openTime = parse(oanda_ob['openTime'])\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return str(vars(self))\n",
    "\n",
    "    @classmethod\n",
    "    def TradeFromAPI(cls, api_object):\n",
    "        return OandaTrade(api_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'unrealizedPL': -0.2538, 'currentUnits': 1, 'trade_id': 1719, 'openTime': datetime.datetime(2022, 5, 12, 14, 29, 35, 16909, tzinfo=tzutc())}\n",
      "{'unrealizedPL': -1.5216, 'currentUnits': 1, 'trade_id': 1715, 'openTime': datetime.datetime(2022, 5, 12, 14, 22, 4, 285644, tzinfo=tzutc())}\n",
      "{'unrealizedPL': -1.6829, 'currentUnits': 1, 'trade_id': 1711, 'openTime': datetime.datetime(2022, 5, 12, 14, 21, 59, 354979, tzinfo=tzutc())}\n",
      "{'unrealizedPL': -4.8733, 'currentUnits': 1, 'trade_id': 1707, 'openTime': datetime.datetime(2022, 5, 12, 14, 13, 49, 764368, tzinfo=tzutc())}\n",
      "{'unrealizedPL': 0.5276, 'currentUnits': 1, 'trade_id': 1703, 'openTime': datetime.datetime(2022, 5, 12, 14, 9, 46, 88614, tzinfo=tzutc())}\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "class OandaAPI():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()    \n",
    "    \n",
    "    def make_request(self, url, params={}, added_headers=None, verb='get', data=None, code_ok=200):\n",
    "\n",
    "        headers = SECURE_HEADER\n",
    "                \n",
    "        try:\n",
    "            response = None\n",
    "            \n",
    "            if verb == 'post':\n",
    "                response = self.session.post(url,params=params,headers=headers,data=data)\n",
    "            elif verb == 'put':\n",
    "                response = self.session.put(url,params=params,headers=headers,data=data)\n",
    "            else:\n",
    "                response = self.session.get(url,params=params,headers=headers,data=data)\n",
    "\n",
    "            status_code = response.status_code\n",
    "\n",
    "            if status_code == code_ok:\n",
    "                json_response = response.json()\n",
    "                return status_code, json_response\n",
    "            \n",
    "            else:\n",
    "                return status_code, None  \n",
    "                \n",
    "        except:\n",
    "            print('make_request error 1')\n",
    "            \n",
    "            slack_msg('make_request error 1')\n",
    "            \n",
    "            time.sleep(70)\n",
    "            \n",
    "            try:\n",
    "                response = None\n",
    "                if verb == 'post':\n",
    "                    response = self.session.post(url,params=params,headers=headers,data=data)\n",
    "                elif verb == 'put':\n",
    "                    response = self.session.put(url,params=params,headers=headers,data=data)\n",
    "                else:\n",
    "                    response = self.session.get(url,params=params,headers=headers,data=data)\n",
    "                status_code = response.status_code\n",
    "                \n",
    "                if status_code == code_ok:\n",
    "                    json_response = response.json()\n",
    "                    return status_code, json_response\n",
    "            \n",
    "                else:\n",
    "                    return status_code, None  \n",
    "            except:\n",
    "                print(\"make_request error 2\")\n",
    "                \n",
    "                slack_msg('make_request error 2')\n",
    "                return 400, None\n",
    "                \n",
    "    #get candles from server and change to df with candles_to_df\n",
    "    \n",
    "    def fetch_candles(self, pair_name, count=None, granularity=\"M1\", date_from=None, date_to=None, as_df=True):\n",
    "        \n",
    "        '''Function to fetch candles from server through count or time'''\n",
    "        \n",
    "        url = f\"{OANDA_URL}/instruments/{pair_name}/candles\"\n",
    "        \n",
    "        # mba is master of biz admin/ mid bid ask\n",
    "        \n",
    "        params = dict(\n",
    "            granularity = granularity,\n",
    "            price = \"MBA\"\n",
    "        )\n",
    "        \n",
    "        # server accepts timestamp\n",
    "        \n",
    "        if date_from is not None and date_to is not None:\n",
    "            params['to'] = int(date_to.timestamp())\n",
    "            params['from'] = int(date_from.timestamp())\n",
    "            \n",
    "        elif count is not None:\n",
    "            params['count'] = count\n",
    "        else:\n",
    "            params['count'] = 11\n",
    "            \n",
    "        try:\n",
    "            status_code, data = self.make_request(url, params=params)\n",
    "        \n",
    "        except:\n",
    "            \n",
    "            try:\n",
    "                print('GET error 1, sleep')\n",
    "                # pushbullet_message('GET error 1')\n",
    "                slack_msg('GET error 1')\n",
    "                time.sleep(5)\n",
    "                status_code, data = self.make_request(url, params=params)\n",
    "                \n",
    "            except:\n",
    "                print('GET error 2, sleep')\n",
    "                # pushbullet_message('GET error 2')\n",
    "                slack_msg('GET error 2')\n",
    "                time.sleep(5)\n",
    "                status_code, data = self.make_request(url, params=params)\n",
    "\n",
    "        if status_code != 200:\n",
    "            print(status_code)\n",
    "            # pushbullet_message('GET error 3',status_code)\n",
    "            slack_msg(f'GET error 3, status code: {status_code}')\n",
    "            return status_code, None\n",
    "\n",
    "        return status_code, OandaAPI.candles_to_df(data['candles'])\n",
    "\n",
    "    def place_trade(self,pair, units, diff_tpsl_threshold, pred_price, take_profit = None, stop_loss = None):\n",
    "        \n",
    "        '''Function to place trade'''\n",
    "        \n",
    "        url = f\"{OANDA_URL}/accounts/{ACCOUNT_ID}/orders\"\n",
    "\n",
    "        data = {\n",
    "            \"order\": {\n",
    "                \"units\": units,\n",
    "                \"instrument\": pair,\n",
    "                \"timeInForce\": \"FOK\", # fork or kill\n",
    "                \"type\": \"MARKET\", # we are carrying out market orders for this strat\n",
    "                \"positionFill\": \"DEFAULT\",\n",
    "\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        #make a trade with the data above\n",
    "        \n",
    "        status_code, json_data = self.make_request(url,verb = 'post',data = json.dumps(data), code_ok = 201)\n",
    "        \n",
    "        if status_code!=201:\n",
    "            \n",
    "            time.sleep(1)\n",
    "            \n",
    "            data = {\n",
    "            \"order\": {\n",
    "                \"units\": units,\n",
    "                \"instrument\": pair,\n",
    "                \"timeInForce\": \"FOK\",\n",
    "                \"type\": \"MARKET\",\n",
    "                \"positionFill\": \"DEFAULT\",\n",
    "\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            status_code, json_data = self.make_request(url, verb='post', data=json.dumps(data), code_ok=201)\n",
    "\n",
    "            if status_code != 201:\n",
    "                print('error making trade',status_code, pair, dt.datetime.now())\n",
    "                slack_msg(f'error making trade, status code: {status_code}, pair: {pair}, time: {dt.datetime.now().replace(microsecond=0,second=0)}')\n",
    "            \n",
    "        trade_id = None\n",
    "        \n",
    "        tp_ok = True\n",
    "        sl_ok = True\n",
    "        \n",
    "        # once the trade is in, we have to follow up with tp and sl\n",
    "        \n",
    "        if 'orderFillTransaction' in json_data and 'tradeOpened' in json_data['orderFillTransaction']:\n",
    "            \n",
    "            trade_id = int(json_data['orderFillTransaction']['tradeOpened']['tradeID'])\n",
    "            price_str = json_data['orderFillTransaction']['price']\n",
    "            units = int(json_data['orderFillTransaction']['tradeOpened']['units'])\n",
    "            print(f' trade done: {pair}, units: {units}, price: {price_str}, time: {dt.datetime.now().replace(microsecond=0,second=0)}')\n",
    "            \n",
    "            if take_profit is not None:\n",
    "                \n",
    "                tp_diff = round(abs(float(price_str) - pred_price),3)\n",
    "                \n",
    "                # risk control\n",
    "                \n",
    "                if tp_diff > diff_tpsl_threshold:\n",
    "                    tp_diff = diff_tpsl_threshold\n",
    "                \n",
    "                if units>0:\n",
    "                    \n",
    "                    tp_price = float(price_str) + tp_diff\n",
    "                    \n",
    "                    if (self.set_sl_tp(tp_price, 'TAKE_PROFIT', trade_id) == False):\n",
    "                        tp_ok = False\n",
    "                        \n",
    "                if units <0:\n",
    "                    \n",
    "                    tp_price = float(price_str) - tp_diff\n",
    "                    \n",
    "                    if (self.set_sl_tp(tp_price, 'TAKE_PROFIT', trade_id) == False):\n",
    "                        tp_ok = False\n",
    "                \n",
    "            if stop_loss is not None:\n",
    "                \n",
    "                sl_diff = round(abs(float(price_str) - pred_price),3)\n",
    "                \n",
    "                # risk control\n",
    "                \n",
    "                if sl_diff > diff_tpsl_threshold:\n",
    "                    sl_diff = diff_tpsl_threshold\n",
    "                \n",
    "                if units>0:\n",
    "                    \n",
    "                    sl_price = float(price_str) - sl_diff\n",
    "                    \n",
    "                    if (self.set_sl_tp(sl_price, 'STOP_LOSS', trade_id) == False):\n",
    "                        sl_ok = False\n",
    "                        \n",
    "                if units<0:\n",
    "                    \n",
    "                    sl_price = float(price_str) + sl_diff\n",
    "                    \n",
    "                    if (self.set_sl_tp(sl_pric, 'STOP_LOSS', trade_id) == False):\n",
    "                        sl_ok = False\n",
    "                        \n",
    "            if ((tp_ok==True) & (sl_ok==True)):\n",
    "                \n",
    "            # slack message to notify that trade has been done\n",
    "            \n",
    "                slack_msg(f'trade done: {pair}, time: {dt.datetime.now().replace(microsecond=0,second=0)}, price: {float(price_str)}, units: {units}, tp: {tp_price}, sl: {sl_price}, diff: {tp_diff}')\n",
    "                \n",
    "        return trade_id, tp_ok, sl_ok, tp_price, sl_price \n",
    "            \n",
    "    def set_sl_tp(self,price, order_type, trade_id):\n",
    "        \n",
    "        url = f'{OANDA_URL}/accounts/{ACCOUNT_ID}/orders'\n",
    "                          \n",
    "        data = {'order':\n",
    "                {'timeinForce' : 'GTC',\n",
    "                 'price': str(price),\n",
    "                 'type': order_type,\n",
    "                 'tradeID': str(trade_id)   \n",
    "                }\n",
    "        }\n",
    "                          \n",
    "        status_code, json_data = self.make_request(url, verb = 'post', data = json.dumps(data), code_ok = 201)\n",
    "                          \n",
    "        if status_code != 201:\n",
    "                          \n",
    "            time.sleep(1)\n",
    "                          \n",
    "            data = {'order':\n",
    "                    {'timeinForce' : 'GTC',\n",
    "                     'price': str(price),\n",
    "                     'type': order_type,\n",
    "                     'tradeID': str(trade_id)   \n",
    "                    }\n",
    "            }\n",
    "\n",
    "            status_code, json_data = self.make_request(url, verb = 'post', data = json.dumps(data), code_ok = 201)\n",
    "                          \n",
    "            if status_code != 201:\n",
    "                slack_msg(f'error making tp_sl, status code: {status_code}, pair: {pair}, time: {dt.datetime.now().replace(microsecond=0,second=0)}')\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "                       \n",
    "    #run request to close trade\n",
    "                          \n",
    "    def close_trade(self,trade_id):\n",
    "        url = f\"{OANDA_URL}/accounts/{ACCOUNT_ID}/trades/{trade_id}/close\"\n",
    "        status_code, json_data = self.make_request(url, verb='put', code_ok=200)\n",
    "        if status_code !=200:\n",
    "            print(status_code)\n",
    "            return False\n",
    "        print ('close trade')\n",
    "        return True\n",
    "    \n",
    "    #returns all open trades as OandaTrade object\n",
    "                          \n",
    "    def open_trades(self):\n",
    "        url = f\"{OANDA_URL}/accounts/{ACCOUNT_ID}/openTrades\"\n",
    "        status_code, data = self.make_request(url)\n",
    "        \n",
    "        if status_code !=200:\n",
    "            return [], False\n",
    "        #could be that the code works but we dont have any open trades. will still return false. so return true as long as \n",
    "        #'trades' not in data\n",
    "        if 'trades' not in data:\n",
    "            return [], True\n",
    "        \n",
    "        trades = [OandaTrade.TradeFromAPI(x) for x in data['trades']]\n",
    "        \n",
    "        return trades,True\n",
    "    \n",
    "    #change candles from fetch candles to a dataframe\n",
    "                          \n",
    "    @classmethod\n",
    "    def candles_to_df(cls, json_data):\n",
    "        prices = ['mid', 'bid', 'ask']\n",
    "        ohlc = ['o', 'h', 'l', 'c']\n",
    "\n",
    "        our_data = []\n",
    "        for candle in json_data:\n",
    "            if candle['complete'] == False:\n",
    "                continue\n",
    "            new_dict = {}\n",
    "            new_dict['time'] = candle['time']\n",
    "            new_dict['volume'] = candle['volume']\n",
    "            for price in prices:\n",
    "                for oh in ohlc:\n",
    "                    new_dict[f\"{price}_{oh}\"] = float(candle[price][oh])\n",
    "            our_data.append(new_dict)\n",
    "        df = pd.DataFrame.from_dict(our_data)\n",
    "        \n",
    "        return df\n",
    "                          \n",
    "    prices = ['mid', 'bid', 'ask']\n",
    "    ohlc = ['o', 'h', 'l', 'c']\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    api = OandaAPI()\n",
    "    #res, df = api.fetch_candles(\"EUR_USD\",date_to = time_utc(), date_from = time_utc()- dt.timedelta(35))\n",
    "    #display(df.head())\n",
    "    #print(api.last_complete_candle('EUR_USD'))\n",
    "    trades, ok = api.open_trades()\n",
    "    if ok == True:\n",
    "         [print(t) for t in trades]\n",
    "         print(ok)\n",
    "\n",
    "#api=OandaAPI()\n",
    "#api.set_sl_tp(price = 1.5,'TAKE_PROFIT','1088')\n",
    "#api = OandaAPI()\n",
    "#api.place_trade('GBP_JPY',units = 50,price = 155.6337,take_profit = 155.6637, stop_loss= 155.3350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save candles to file\n",
    "\n",
    "INCREMENTS = {'M1':1, 'M5':5, 'H1':60,\n",
    "             'D':1440}\n",
    "\n",
    "pair = 'XAU_USD'\n",
    "granularity = 'M1'\n",
    "\n",
    "def create_file(pair,granularity,api,date_from,end_date,time_from = None,time_to = None):\n",
    "    \n",
    "    candle_count = 2000\n",
    "    \n",
    "    time_step = INCREMENTS[granularity] * candle_count\n",
    "    \n",
    "    candle_dfs = []\n",
    "    \n",
    "    while date_from < end_date:\n",
    "        date_to = date_from + dt.timedelta(minutes = time_step)\n",
    "    \n",
    "        if date_to > end_date:\n",
    "            date_to = end_date\n",
    "        \n",
    "        code, df = api.fetch_candles(pair, granularity=granularity, date_from = date_from, date_to = date_to, as_df = True)\n",
    "        \n",
    "        if df is not None and df.empty == False:\n",
    "            candle_dfs.append(df)\n",
    "            \n",
    "        elif code != 200:\n",
    "            print ('ERROR', pair, granularity, date_from,date_to)\n",
    "            \n",
    "            slack_msg(f'create_file error 1, code :{code}, pair: {pair}, date from : {date_from}, date to : {date_to}')\n",
    "            \n",
    "            break\n",
    "            \n",
    "        date_from = date_to\n",
    "        \n",
    "    final_df = pd.concat(candle_dfs)\n",
    "    \n",
    "    final_df.drop_duplicates(subset = 'time', inplace=True)\n",
    "    \n",
    "    final_df.sort_values(by = 'time', inplace=True)\n",
    "    \n",
    "    # time_to input is included if we want to save file under diff name\n",
    "    \n",
    "    if time_to:\n",
    "        \n",
    "        final_df.to_pickle(f'{get_his_data_filename(pair,granularity,time_from,time_to)}')\n",
    "  \n",
    "    else:\n",
    "        final_df.to_pickle(f'{get_his_data_filename(pair,granularity)}')\n",
    "    \n",
    "    print (f'{pair} {granularity} {final_df.iloc[0].time} {final_df.iloc[-1].time} {dt.datetime.now().hour}:{dt.datetime.now().minute}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_library = {\n",
    "    'USD' :  { \"pair\" :  'XAU_USD', \"units\": 1, \"pip\": 0.1, \"pips_bound\": 5},\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"./datasets/lstm_model_9_transform\")\n",
    "\n",
    "##model = load_model(\"lstm_bot_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_trading_strat(diff_tpsl_threshold, minimum_trade_threshold):\n",
    "    \n",
    "    '''This function carries out our strat and places trade'''\n",
    "    \n",
    "    api= OandaAPI()\n",
    "    \n",
    "    #log_df is log of all trades done\n",
    "    \n",
    "    pkl1 = './datasets/event_trader_log.pkl'\n",
    "    with open(pkl1, \"rb\") as fh:\n",
    "        log_df = pickle.load(fh)\n",
    "    \n",
    "    #up_coming_events_df is df for upcoming events we will trade\n",
    "    \n",
    "    up_coming_events_df = pd.read_pickle('./datasets/up_coming_events.pkl')\n",
    "    \n",
    "    up_coming_events_df['get_candle_time']=astimezone(dt.datetime.now().replace(microsecond=0,second=0))\n",
    "    # -> for testing\n",
    "    \n",
    "    # extracting x and y to min max scale newest candles later on\n",
    "    \n",
    "    x_train_unscaled = pd.read_pickle('./datasets/x_train_candles_unscaled.pkl')\n",
    "    \n",
    "    y_train_unscaled = pd.read_pickle('./datasets/y_train_candles_unscaled.pkl')\n",
    "    \n",
    "    time_now = dt.datetime.now(pytz.timezone('Asia/Singapore')).replace(microsecond=0,second=0)\n",
    "    \n",
    "    # if time now is exactly our event get candles time\n",
    "    \n",
    "    if (up_coming_events_df['get_candle_time'] == time_now).any() == True:\n",
    "        \n",
    "        # check that we have never carried out trade for that event before\n",
    "        \n",
    "        if (log_df['get_candle_time'] == time_now).any() == False:\n",
    "            \n",
    "            code, candle_stick_df = api.fetch_candles(pair_name='XAU_USD', count = 11)\n",
    "            \n",
    "            candle_stick_df['time'] = candle_stick_df['time'].apply(parse_utc_to_sg)\n",
    "            \n",
    "            def run_ml(log_df = log_df):\n",
    "\n",
    "                lstm_x_df = pd.DataFrame(np.array(candle_stick_df['mid_c']).reshape(1,-1))\n",
    "\n",
    "                x_train_to_fit = x_train_unscaled.append(lstm_x_df)\n",
    "\n",
    "                ms = MinMaxScaler()\n",
    "                ms2 = MinMaxScaler()\n",
    "\n",
    "                x_train_sc = ms.fit_transform(x_train_to_fit)\n",
    "                y_train_sc = ms2.fit_transform(pd.DataFrame(y_train_unscaled))\n",
    "\n",
    "                newest_candles_sc = x_train_sc[-1]\n",
    "\n",
    "                newest_candles_2 = np.array(newest_candles_sc).reshape(1, newest_candles_sc.shape[0],1)\n",
    "\n",
    "                predictions = model.predict(newest_candles_2)\n",
    "\n",
    "                predicted_price = ms2.inverse_transform(predictions)\n",
    "\n",
    "                pred_price = predicted_price[0][0]\n",
    "\n",
    "                curr = 'USD'\n",
    "\n",
    "                pair = data_library[curr]['pair']\n",
    "\n",
    "                last_price = candle_stick_df.iloc[-1]['mid_c']\n",
    "\n",
    "                if pred_price> last_price:\n",
    "                    decision = 1\n",
    "\n",
    "                elif pred_price< last_price:\n",
    "                    decision = -1\n",
    "\n",
    "                difference = abs(pred_price - last_price)\n",
    "\n",
    "                if difference > minimum_trade_threshold:\n",
    "\n",
    "                    units = decision * data_library[curr]['units']\n",
    "\n",
    "                    trade_id, tp_ok, sl_ok, tp_price, sl_price = api.place_trade(pair, units, \n",
    "                                                        diff_tpsl_threshold = diff_tpsl_threshold, \n",
    "                                                        pred_price = pred_price,\n",
    "                                                        take_profit = True, stop_loss = True)\n",
    "\n",
    "                    # log_cols = event_df.columns.to_list() + ['price','tp','sl','retrained_model']\n",
    "\n",
    "                    log_temp = pd.DataFrame(columns = up_coming_events_df.columns)\n",
    "                    \n",
    "                    log_temp = log_temp.append(up_coming_events_df[up_coming_events_df['get_candle_time']==time_now])\n",
    "                    \n",
    "                    #display(up_coming_events_df[up_coming_events_df['get_candle_time']==time_now])\n",
    "                    \n",
    "                    log_temp['price'] = pred_price\n",
    "                    \n",
    "                    log_temp['tp'] = tp_price\n",
    "\n",
    "                    log_temp['sl'] = sl_price\n",
    "\n",
    "                    log_temp['retrained_model'] = 0\n",
    "                    \n",
    "                    log_df = log_df.append(log_temp)\n",
    "                    \n",
    "                    log_df.reset_index(inplace=True,drop=True)\n",
    "                    \n",
    "                    display(log_df.tail(3))\n",
    "                    \n",
    "                    log_df.to_pickle('./datasets/event_trader_log.pkl')\n",
    "                    \n",
    "            # we have to make sure that the last candle is the correct time that we want\n",
    "            # if not we wait a few seconds and retrieve again\n",
    "            \n",
    "            if candle_stick_df['time'].iloc[-1] == time_now-dt.timedelta(minutes=1):\n",
    "                run_ml()\n",
    "                \n",
    "            else: \n",
    "                time.sleep(1)\n",
    "                \n",
    "                code, candle_stick_df = api.fetch_candles(pair_name='XAU_USD', count = 11)\n",
    "            \n",
    "                candle_stick_df['time'] = candle_stick_df['time'].apply(parse_utc_to_sg)\n",
    "                \n",
    "                if candle_stick_df['time'].iloc[-1] == time_now-dt.timedelta(minutes=1):\n",
    "            \n",
    "                    run_ml()\n",
    "                \n",
    "                else:\n",
    "                    time.sleep(1)\n",
    "                \n",
    "                    code, candle_stick_df = api.fetch_candles(pair_name='XAU_USD', count = 11)\n",
    "            \n",
    "                    candle_stick_df['time'] = candle_stick_df['time'].apply(parse_utc_to_sg)\n",
    "                \n",
    "                    if candle_stick_df['time'].iloc[-1] == time_now-dt.timedelta(minutes=1):\n",
    "            \n",
    "                        run_ml()\n",
    "                \n",
    "                    else:\n",
    "                        print(f' wrong_time: {candle_stick_df[\"time\"].iloc[-1]}, expected_time: {time_now-dt.timedelta(minutes=1)}')\n",
    "                              \n",
    "                        slack_msg(f' wrong_time: {candle_stick_df[\"time\"].iloc[-1]}, expected_time: {time_now-dt.timedelta(minutes=1)}')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.read_pickle('./datasets/event_trader_log.pkl').iloc[0].to_pickle('./datasets/event_trader_log.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_cols = up_coming_events_df.columns.to_list() +['price','tp','sl','retrained_model']\n",
    "# log_df = pd.DataFrame(columns=log_cols)\n",
    "# log_df.to_pickle('event_trader_log.pkl')\n",
    "# log_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log_df = pd.read_pickle('event_trader_log.pkl')\n",
    "#log_df['retrained_model']=0\n",
    "#log_df.to_pickle('event_trader_log.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " trade done: XAU_USD, units: 1, price: 1838.920, time: 2022-05-12 22:31:00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>event_date_time</th>\n",
       "      <th>event_time_holder</th>\n",
       "      <th>curr</th>\n",
       "      <th>impact</th>\n",
       "      <th>event</th>\n",
       "      <th>previous</th>\n",
       "      <th>forecast</th>\n",
       "      <th>actual</th>\n",
       "      <th>get_candle_time</th>\n",
       "      <th>price</th>\n",
       "      <th>tp</th>\n",
       "      <th>sl</th>\n",
       "      <th>retrained_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wed</td>\n",
       "      <td>2022-05-11 20:30:00+08:00</td>\n",
       "      <td>20:30</td>\n",
       "      <td>USD</td>\n",
       "      <td>high</td>\n",
       "      <td>CPI m/m</td>\n",
       "      <td>1.2%</td>\n",
       "      <td>0.2%</td>\n",
       "      <td>0.3%</td>\n",
       "      <td>2022-05-12 22:09:00+08:00</td>\n",
       "      <td>1865.193726</td>\n",
       "      <td>1848.855</td>\n",
       "      <td>1828.855</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mon</td>\n",
       "      <td>2022-05-02 22:00:00+08:00</td>\n",
       "      <td>16:00</td>\n",
       "      <td>USD</td>\n",
       "      <td>high</td>\n",
       "      <td>ISM Manufacturing PMI</td>\n",
       "      <td>57.1</td>\n",
       "      <td>57.5</td>\n",
       "      <td></td>\n",
       "      <td>2022-05-12 22:29:00+08:00</td>\n",
       "      <td>1867.235229</td>\n",
       "      <td>1849.416</td>\n",
       "      <td>1829.416</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mon</td>\n",
       "      <td>2022-05-02 22:00:00+08:00</td>\n",
       "      <td>16:00</td>\n",
       "      <td>USD</td>\n",
       "      <td>high</td>\n",
       "      <td>ISM Manufacturing PMI</td>\n",
       "      <td>57.1</td>\n",
       "      <td>57.5</td>\n",
       "      <td></td>\n",
       "      <td>2022-05-12 22:31:00+08:00</td>\n",
       "      <td>1866.916138</td>\n",
       "      <td>1848.92</td>\n",
       "      <td>1828.92</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  day_of_week            event_date_time event_time_holder curr impact  \\\n",
       "1         Wed  2022-05-11 20:30:00+08:00             20:30  USD   high   \n",
       "2         Mon  2022-05-02 22:00:00+08:00             16:00  USD   high   \n",
       "3         Mon  2022-05-02 22:00:00+08:00             16:00  USD   high   \n",
       "\n",
       "                   event previous forecast actual           get_candle_time  \\\n",
       "1                CPI m/m     1.2%     0.2%   0.3% 2022-05-12 22:09:00+08:00   \n",
       "2  ISM Manufacturing PMI     57.1     57.5        2022-05-12 22:29:00+08:00   \n",
       "3  ISM Manufacturing PMI     57.1     57.5        2022-05-12 22:31:00+08:00   \n",
       "\n",
       "         price        tp        sl retrained_model  \n",
       "1  1865.193726  1848.855  1828.855               1  \n",
       "2  1867.235229  1849.416  1829.416               1  \n",
       "3  1866.916138   1848.92   1828.92               0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running... 22:31\n",
      "running... 22:31\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/f3/bfm2k8td0555p4d828xyr9yw0000gn/T/ipykernel_88506/3812226818.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mrun_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/f3/bfm2k8td0555p4d828xyr9yw0000gn/T/ipykernel_88506/3812226818.py\u001b[0m in \u001b[0;36mrun_collection\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mevent_trading_strat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34mf'running... {dt.datetime.now().hour}:{dt.datetime.now().minute}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "INCREMENTS = {'M1':1,}\n",
    "\n",
    "#for each pair, create a file\n",
    "def run_collection():\n",
    "    while True:\n",
    "        \n",
    "        event_trading_strat(10,5)\n",
    "        \n",
    "        time.sleep(3)\n",
    "        \n",
    "        print (f'running... {dt.datetime.now().hour}:{dt.datetime.now().minute}')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>event_date_time</th>\n",
       "      <th>event_time_holder</th>\n",
       "      <th>curr</th>\n",
       "      <th>impact</th>\n",
       "      <th>event</th>\n",
       "      <th>previous</th>\n",
       "      <th>forecast</th>\n",
       "      <th>actual</th>\n",
       "      <th>get_candle_time</th>\n",
       "      <th>price</th>\n",
       "      <th>tp</th>\n",
       "      <th>sl</th>\n",
       "      <th>retrained_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wed</td>\n",
       "      <td>2022-05-11 20:30:00+08:00</td>\n",
       "      <td>20:30</td>\n",
       "      <td>USD</td>\n",
       "      <td>high</td>\n",
       "      <td>CPI m/m</td>\n",
       "      <td>1.2%</td>\n",
       "      <td>0.2%</td>\n",
       "      <td>0.3%</td>\n",
       "      <td>2022-05-12 22:09:00+08:00</td>\n",
       "      <td>1865.193726</td>\n",
       "      <td>1848.855</td>\n",
       "      <td>1828.855</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wed</td>\n",
       "      <td>2022-05-11 20:30:00+08:00</td>\n",
       "      <td>20:30</td>\n",
       "      <td>USD</td>\n",
       "      <td>high</td>\n",
       "      <td>CPI m/m</td>\n",
       "      <td>1.2%</td>\n",
       "      <td>0.2%</td>\n",
       "      <td>0.3%</td>\n",
       "      <td>2022-05-12 22:09:00+08:00</td>\n",
       "      <td>1865.193726</td>\n",
       "      <td>1848.855</td>\n",
       "      <td>1828.855</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mon</td>\n",
       "      <td>2022-05-02 22:00:00+08:00</td>\n",
       "      <td>16:00</td>\n",
       "      <td>USD</td>\n",
       "      <td>high</td>\n",
       "      <td>ISM Manufacturing PMI</td>\n",
       "      <td>57.1</td>\n",
       "      <td>57.5</td>\n",
       "      <td></td>\n",
       "      <td>2022-05-12 22:29:00+08:00</td>\n",
       "      <td>1867.235229</td>\n",
       "      <td>1849.416</td>\n",
       "      <td>1829.416</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mon</td>\n",
       "      <td>2022-05-02 22:00:00+08:00</td>\n",
       "      <td>16:00</td>\n",
       "      <td>USD</td>\n",
       "      <td>high</td>\n",
       "      <td>ISM Manufacturing PMI</td>\n",
       "      <td>57.1</td>\n",
       "      <td>57.5</td>\n",
       "      <td></td>\n",
       "      <td>2022-05-12 22:31:00+08:00</td>\n",
       "      <td>1866.916138</td>\n",
       "      <td>1848.92</td>\n",
       "      <td>1828.92</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  day_of_week            event_date_time event_time_holder curr impact  \\\n",
       "0         Wed  2022-05-11 20:30:00+08:00             20:30  USD   high   \n",
       "1         Wed  2022-05-11 20:30:00+08:00             20:30  USD   high   \n",
       "2         Mon  2022-05-02 22:00:00+08:00             16:00  USD   high   \n",
       "3         Mon  2022-05-02 22:00:00+08:00             16:00  USD   high   \n",
       "\n",
       "                   event previous forecast actual           get_candle_time  \\\n",
       "0                CPI m/m     1.2%     0.2%   0.3% 2022-05-12 22:09:00+08:00   \n",
       "1                CPI m/m     1.2%     0.2%   0.3% 2022-05-12 22:09:00+08:00   \n",
       "2  ISM Manufacturing PMI     57.1     57.5        2022-05-12 22:29:00+08:00   \n",
       "3  ISM Manufacturing PMI     57.1     57.5        2022-05-12 22:31:00+08:00   \n",
       "\n",
       "         price        tp        sl retrained_model  \n",
       "0  1865.193726  1848.855  1828.855               1  \n",
       "1  1865.193726  1848.855  1828.855               1  \n",
       "2  1867.235229  1849.416  1829.416               1  \n",
       "3  1866.916138   1848.92   1828.92               0  "
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_df=pd.read_pickle('./datasets/event_trader_log.pkl')\n",
    "log_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_retrain_candles(pair,granularity):\n",
    "    \n",
    "    api = OandaAPI()\n",
    "    \n",
    "    x_train_unscaled = pd.read_pickle('./datasets/x_train_candles_unscaled.pkl')\n",
    "    \n",
    "    y_train_unscaled = pd.read_pickle('./datasets/y_train_candles_unscaled.pkl')\n",
    "    \n",
    "    log_df = pd.read_pickle('./datasets/event_trader_log.pkl')\n",
    "    \n",
    "    untrained_df = log_df[log_df['retrained_model']==0]\n",
    "    \n",
    "    for i in untrained_df.index:\n",
    "        \n",
    "        if i is not None:\n",
    "            \n",
    "            date_from = astimezone_utc(log_df.loc[i]['event_date_time'])-dt.timedelta(minutes = 1)\n",
    "\n",
    "            date_to = astimezone_utc(log_df.loc[i]['event_date_time'])+dt.timedelta(minutes = 10)\n",
    "\n",
    "            create_file('XAU_USD','M1',api,date_from,date_to)\n",
    "\n",
    "            data_df = pd.read_pickle(f'{get_his_data_filename(pair,granularity)}')\n",
    "\n",
    "            if data_df.shape[0]== 11:\n",
    "\n",
    "                x_train = data_df['mid_c'][:10]\n",
    "                y_train = data_df['mid_c'].iloc[-1]\n",
    "\n",
    "                lstm_x_df = pd.DataFrame(np.array(x_train).reshape(1,-1))\n",
    "                lstm_y_df = pd.Series(y_train)\n",
    "\n",
    "                x_train_unscaled = x_train_unscaled.append(lstm_x_df)\n",
    "                y_train_unscaled = y_train_unscaled.append(lstm_y_df)\n",
    "\n",
    "                log_df.loc[i,'retrained_model'] =1\n",
    "\n",
    "                train_list.extend([i])\n",
    "\n",
    "            else:\n",
    "                untrained_list.extend([i])\n",
    "                \n",
    "    x_train_unscaled.to_pickle('./datasets/x_train_unscaled_retrain.pkl')\n",
    "    y_train_unscaled.to_pickle('./datasets/y_train_unscaled_retrain.pkl')\n",
    "    \n",
    "    log_df.to_pickle('./datasets/event_trader_log.pkl')\n",
    "    \n",
    "    display(train_list,untrained_list)\n",
    "    \n",
    "def retrain_model(pair,granularity):\n",
    "    \n",
    "    x_train_unscaled_retrain = pd.read_pickle('./datasets/x_train_unscaled_retrain.pkl')\n",
    "    y_train_unscaled_retrain = pd.read_pickle('./datasets/y_train_unscaled_retrain.pkl')\n",
    "    \n",
    "    ms = MinMaxScaler()\n",
    "    ms2 = MinMaxScaler()\n",
    "\n",
    "    x_train_sc = ms.fit_transform(x_train_unscaled_retrain)\n",
    "    y_train_sc = ms2.fit_transform(pd.DataFrame(y_train_unscaled_retrain))\n",
    "    \n",
    "    x_train_2 = np.array(x_train_sc).reshape(x_train_sc.shape[0], x_train_sc.shape[1],1)\n",
    "    \n",
    "    y_train_1 = np.array(y_train_sc)\n",
    "    y_train_2 = y_train_1.reshape(-1,1)\n",
    "    \n",
    "    display(x_train_2.shape, y_train_2.shape)\n",
    "    \n",
    "    #initializing the model\n",
    "\n",
    "    tf.random.set_seed(0)\n",
    "\n",
    "    model= Sequential()\n",
    "\n",
    "    #First Input layer and LSTM layer with 20% dropout\n",
    "    model.add(LSTM(10,return_sequences=True,input_shape=(10,1)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    #Input layer and LSTM layer with 20% dropout\n",
    "    model.add(LSTM(10,return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    #Input layer and LSTM layer with 20% dropout\n",
    "    model.add(LSTM(10,return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    #Input layer and LSTM layer with 20% dropout\n",
    "    model.add(LSTM(10,return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(units=1))\n",
    "    \n",
    "    model.compile(optimizer='adam',loss='mean_squared_error')\n",
    "\n",
    "    #es = EarlyStopping(patience = 5)\n",
    "\n",
    "    #fitting the network\n",
    "    \n",
    "    history = model.fit(x_train_2,y_train_2,epochs=1,\n",
    "                    batch_size=10)\n",
    "    \n",
    "    sample_pred = model.predict(x_train_2[-1].reshape(1,10,1))\n",
    "    \n",
    "    sample_pred_2 = ms2.inverse_transform(sample_pred)\n",
    "    \n",
    "    display(sample_pred_2)\n",
    "    \n",
    "    model.save(\"./datasets/lstm_bot_model\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XAU_USD M1 2022-05-02T13:59:00.000000000Z 2022-05-02T14:09:00.000000000Z 22:31\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# collect candles for weekly retrain of model\n",
    "\n",
    "train_list = []\n",
    "untrained_list = []\n",
    "\n",
    "collect_retrain_candles('XAU_USD','M1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3039, 10, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(3039, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3039 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-12 22:31:33.838424: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_standard_lstm_208187_208672' and '__inference___backward_standard_lstm_208187_208672_specialized_for_StatefulPartitionedCall_1_at___inference_distributed_function_209877' both implement 'lstm_711ce433-4a5e-4fb4-bb40-d68b398c9025' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  10/3039 [..............................] - ETA: 31:01"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/f3/bfm2k8td0555p4d828xyr9yw0000gn/T/ipykernel_88506/3959268331.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#weekly retrain and save model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgranularity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/f3/bfm2k8td0555p4d828xyr9yw0000gn/T/ipykernel_88506/890429692.py\u001b[0m in \u001b[0;36mretrain_model\u001b[0;34m(pair, granularity)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     history = model.fit(x_train_2,y_train_2,epochs=1,\n\u001b[0;32m---> 99\u001b[0;31m                     batch_size=10)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0msample_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dsi27tf/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dsi27tf/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dsi27tf/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dsi27tf/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dsi27tf/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dsi27tf/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dsi27tf/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dsi27tf/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dsi27tf/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dsi27tf/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dsi27tf/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#weekly retrain and save model\n",
    "\n",
    "model_1 = retrain_model(pair,granularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu, 2022.05.12 13:00, 13:00, JPY, low, Economy Watchers Sentiment, 47.8, 50.9, 50.4\n",
      "\n",
      "Thu, 2022.05.12 14:00, 14:00, GBP, medium, Prelim GDP q/q, 1.3%, 1.0%, 0.8%\n",
      "\n",
      "Thu, 2022.05.12 14:30, 14:30, CHF, low, PPI m/m, 0.8%, 0.9%, 1.3%\n",
      "\n",
      "Thu, 2022.05.12 19:31, 19:31, GBP, low, NIESR GDP Estimate, 0.8%, , 0.3%\n",
      "\n",
      "Thu, 2022.05.12 20:30, 20:30, USD, high, PPI m/m, 1.4%, 0.5%, 0.5%\n",
      "\n",
      "Thu, 2022.05.12 22:30, 22:30, USD, low, Natural Gas Storage, 77B, 82B,  \n",
      "\n",
      "Thu, 2022.05.12 23:35, 23:35, CAD, low, Gov Council Member Gravelle Speaks, , , \n",
      "\n",
      "Fri, 2022.05.13 1:01, 1:01, USD, low, 30-y Bond Auction, 2.82|2.3, , \n",
      "\n",
      "Fri, 2022.05.13 6:30, 6:30, NZD, low, BusinessNZ Manufacturing Index, 53.8, , \n",
      "\n",
      "Fri, 2022.05.13 7:50, 7:50, JPY, low, M2 Money Stock y/y, 3.5%, 3.4%, \n",
      "\n",
      "Fri, 2022.05.13 10:00, 10:00, AUD, low, RBA Deputy Gov Bullock Speaks, , , \n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_next'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/f3/bfm2k8td0555p4d828xyr9yw0000gn/T/ipykernel_88506/519932943.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# scrape events\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mscrape_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/f3/bfm2k8td0555p4d828xyr9yw0000gn/T/ipykernel_88506/3180028221.py\u001b[0m in \u001b[0;36mscrape_events\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mdate_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalendar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonth_abbr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mest_date_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonth\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest_date_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mday\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m'.'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest_date_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mgetEventsCalendar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"calendar?day={date_start}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34mf\"calendar?day={date_end}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/f3/bfm2k8td0555p4d828xyr9yw0000gn/T/ipykernel_88506/2549419912.py\u001b[0m in \u001b[0;36mgetEventsCalendar\u001b[0;34m(start_date, end_date, file_path)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0mscrape_next_day\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'head'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'calendar__pagination--next'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'href'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0mgetEventsCalendar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscrape_next_day\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/f3/bfm2k8td0555p4d828xyr9yw0000gn/T/ipykernel_88506/2549419912.py\u001b[0m in \u001b[0;36mgetEventsCalendar\u001b[0;34m(start_date, end_date, file_path)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Get Date of Event\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m#print(table.find_next('tr', class_ = 'calendar__row--new-day'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mdate_of_events\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'calendar__row--new-day'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'span'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# Regualr Expression to find the 'day of week', 'month' and the 'day'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_next'"
     ]
    }
   ],
   "source": [
    "# scrape events\n",
    "scrape_events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "day_of_week                                Thu\n",
       "event_date_time      2022-05-12 20:30:00+08:00\n",
       "event_time_holder                        20:30\n",
       "curr                                       USD\n",
       "impact                                    high\n",
       "event                                  PPI m/m\n",
       "previous                                  1.4%\n",
       "forecast                                  0.5%\n",
       "actual                                    0.5%\n",
       "get_candle_time      2022-05-12 20:39:00+08:00\n",
       "Name: 0, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "day_of_week                                Thu\n",
       "event_date_time      2022-05-12 20:30:00+08:00\n",
       "event_time_holder                        20:30\n",
       "curr                                       USD\n",
       "impact                                    high\n",
       "event                                  PPI m/m\n",
       "previous                                  1.4%\n",
       "forecast                                  0.5%\n",
       "actual                                    0.5%\n",
       "get_candle_time      2022-05-12 20:39:00+08:00\n",
       "Name: 0, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# preprocess events and log them for our strategy\n",
    "preprocess_events()\n",
    "log_up_coming_events()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
